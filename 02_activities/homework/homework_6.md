# Homework 6: Reflecton

- Due on Saturday, September 21 at 11:59pm
- Weight: 8% of total grade

<br>

**Write**: Reflect on your previous work and how you would adjust to include ethics and inequity components. Total length should be a few paragraphs, no more than one page.


When working with these datasets, it's important to carefully balance ethical considerations and issues of inequity to ensure that data is used responsibly and doesn't inadvertently cause harm or reinforce social biases. From an **ethical** perspective, privacy protection is crucial, especially when handling data from the `customer` and `vendor` tables. The `customer` table contains sensitive information like names, postal codes, and purchase history (`customer_purchases`), while the `vendor_inventory` table holds details such as `original_price` and `quantity`. These types of data must be kept confidential, and access should be restricted based on job roles. For example, managers may only need access to summary sales data, while analysts may need more granular information—but even they shouldn't have access to sensitive customer details unless it's essential and anonymized. Moreover, data like `customer_postal_code` should never be used for unauthorized purposes, such as selling to third parties. It’s equally important to store critical data on local servers instead of the cloud to mitigate the risk of breaches. For customers with the same name, cross-referencing with their postal code or `city` from the `postal_data` table can ensure proper identification, preventing confusion or mismanagement of personal information. When considering **inequity**, we need to be particularly mindful of the potential biases that can arise in the data. The `customer` and `postal_data` tables could reflect demographic imbalances, such as differences in population groups based on race, gender, or economic status (e.g., `median_household_income`). If one demographic (like a certain racial or gender group) is overrepresented in the dataset, the algorithms we develop could end up favoring that group’s behavior, unintentionally disadvantaging others. For example, if the `customer_purchases` table has more data from wealthier or predominantly white neighborhoods, the models could learn to prioritize the purchasing patterns of those groups, leading to biased outcomes. To mitigate this, normalizing the data and ensuring fair representation across different `districts` or income levels is key. Similarly, for vendors, the `vendor_booth_assignments` and `vendor_inventory` data should be reviewed to ensure that small or minority-owned vendors aren't disadvantaged by recommendation systems or pricing strategies that favor larger, more established businesses. Furthermore, sensitive information such as the `original_price` from `vendor_inventory` should remain confidential to protect smaller vendors from losing their competitive edge. If such information were to leak, it could allow larger vendors to undercut prices or dominate the market. By securing these types of data and ensuring they're used fairly, we can help maintain a more balanced and competitive environment. In summary, by implementing strict privacy controls, normalizing data to account for demographic diversity, and ensuring fairness in how algorithms process data, we can handle this information in a way that respects ethical boundaries and promotes equity. This approach will help prevent biases from influencing outcomes and ensure that vendors and customers alike are treated fairly, regardless of their background.
